{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des données de la page 1...\n",
      "Récupération des données de la page 2...\n",
      "Récupération des données de la page 3...\n",
      "Récupération des données de la page 4...\n",
      "Récupération des données de la page 5...\n",
      "Récupération des données de la page 6...\n",
      "Récupération des données de la page 7...\n",
      "Récupération des données de la page 8...\n",
      "Récupération des données de la page 9...\n",
      "Récupération des données de la page 10...\n",
      "Données enregistrées dans le fichier 'VotreNom_PhD1000.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# URL de base pour récupérer les thèses\n",
    "base_url = \"https://theses.fr/resultats?filtres=%255BStatut%253D%2522soutenue%2522%255D&q=*&page=1&nb=10&tri=dateDesc&domaine=theses\"\n",
    "\n",
    "# Fonction pour récupérer les informations d'une page\n",
    "def get_theses_data(page_url):\n",
    "    # Envoyer une requête HTTP GET à la page de thèse\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Erreur de récupération de la page {page_url}\")\n",
    "        return None\n",
    "    \n",
    "    # Utiliser BeautifulSoup pour analyser le contenu HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extraire les informations pertinentes (exemple : titre, auteur, année)\n",
    "    # Vous devrez ajuster les sélecteurs en fonction de la structure de la page HTML du site theses.fr\n",
    "    titles = soup.find_all(\"h3\", class_=\"title\")\n",
    "    authors = soup.find_all(\"span\", class_=\"author\")\n",
    "    dates = soup.find_all(\"span\", class_=\"date\")\n",
    "    \n",
    "    # Collecter les données extraites dans une liste\n",
    "    data = []\n",
    "    for title, author, date in zip(titles, authors, dates):\n",
    "        data.append({\n",
    "            \"Title\": title.get_text(strip=True),\n",
    "            \"Author\": author.get_text(strip=True),\n",
    "            \"Date\": date.get_text(strip=True)\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Fonction pour scrapper les thèses sur plusieurs pages (exemple pour 1000 thèses)\n",
    "def collect_theses_data(num_pages=10):\n",
    "    all_data = []\n",
    "    for page in range(1, num_pages + 1):\n",
    "        page_url = f\"{base_url}?page={page}\"  # Exemple d'URL de page\n",
    "        print(f\"Récupération des données de la page {page}...\")\n",
    "        page_data = get_theses_data(page_url)\n",
    "        if page_data:\n",
    "            all_data.extend(page_data)\n",
    "    \n",
    "    # Convertir les données en DataFrame et sauvegarder en CSV\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(\"/Users/jeannegarcia/Desktop/3A/M.CISEL/Garcia_PhD1000_output.csv\", index=False, encoding='utf-8')\n",
    "    print(\"Données enregistrées dans le fichier 'VotreNom_PhD1000.csv'.\")\n",
    "\n",
    "# Appeler la fonction pour collecter les thèses\n",
    "collect_theses_data(num_pages=10)  # Scraper 10 pages pour 1000 thèses (100 thèses par page)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
